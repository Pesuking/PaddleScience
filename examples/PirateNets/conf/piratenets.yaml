# TODO: make some modification
hydra:
  run:
    # dynamic output directory according to running time and override name
    dir: outputs_piratenets/${now:%Y-%m-%d}/${now:%H-%M-%S}/${hydra.job.override_dirname}
  job:
    name: ${mode} # name of logfile
    chdir: false # keep current working direcotry unchaned
    config:
      override_dirname:
        exclude_keys:
          - TRAIN.checkpoint_path
          - TRAIN.pretrained_model_path
          - EVAL.pretrained_model_path
          - mode
          - output_dir
          - log_freq
  sweep:
    # output directory for multirun
    dir: ${hydra.run.dir}
    subdir: ./

DATA_PATH: "./dataset/allen_cahn.mat"

# general settings
mode: train # running mode: train/eval
seed: 2024
output_dir: ${hydra:run.dir}
log_freq: 100


# model settings
MODEL:
  input_keys: ["t", "x"]
  output_keys: ["u",]
  num_layers: 9
  num_hiddens: 256
  activation: "Tanh"
  alpha: 0
  fourier_scale: 2.0
  fourier_in_features: 2 # t, x
  fourier_out_features: ${MODEL.num_hiddens}
  fourier_trainable: False
  periods:
    x: ["2.0", False] # [period, trainable]
  mu: 1.0
  sigma: 0.1

# optimizer settings
OPTIMIZER:
  initial_learning_rate: 0.001
  decay_rate: 0.9
  decay_steps: 5000
  warmup_steps: 5000

# training settings
TRAIN:
  batch_size: 8192
  epochs: 1
  iters_per_epoch: 300000
  save_freq: 100
  eval_during_train: True
  eval_freq: 100
  lr_scheduler:
    epochs: ${TRAIN.epochs}
    iters_per_epoch: ${TRAIN.iters_per_epoch}
    learning_rate: 1.0e-3
    warmup_epoch: 5000
    gamma: 0.9
    decay_steps: 10
    by_epoch: False
  pretrained_model_path: null
  checkpoint_path: null

# weight settings
WEIGHT:
  weighting_scheme: "NTK"
  casual_tolerance: 1.0
  num_chunks: 32

# evaluation settings
EVAL:
  pretrained_model_path: null
  eval_with_no_grad: true
  batch_size: 128
